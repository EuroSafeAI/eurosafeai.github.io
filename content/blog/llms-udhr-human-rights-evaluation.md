---
title: "When Do Language Models Endorse Limitations on Universal Human Rights Principles?"
slug: "llms-udhr-human-rights-evaluation"
date: "2025-01-10"
authors:
  - "Keenan Samway"
  - "Nicole Miu Takagi"
  - "Rada Mihalcea"
  - "Bernhard Sch√∂lkopf"
  - "Ilias Chalkidis"
  - "Daniel Hershcovich"
  - "Zhijing Jin"
category: "Democracy Defense"
featured: false
summary: "Evaluating how LLMs navigate trade-offs involving the Universal Declaration of Human Rights across eight languages."
---

## Overview

As Large Language Models (LLMs) increasingly mediate global information access with the potential to shape public discourse, their alignment with universal human rights principles becomes important to ensure that these rights are abided by in high-stake AI-mediated interactions.

## Methodology

In this paper, we evaluate how LLMs navigate trade-offs involving the **Universal Declaration of Human Rights (UDHR)**, leveraging 1,152 synthetically generated scenarios across 24 rights articles in eight languages.

## Key Findings

Our analysis of eleven major LLMs reveals systematic biases where models:

1. **Accept limiting Economic, Social, and Cultural rights** more often than Political and Civil rights
2. **Demonstrate significant cross-linguistic variation** with elevated endorsement rates of rights-limiting actions in Chinese and Hindi compared to English or Romanian
3. **Exhibit noticeable differences** between Likert and open-ended responses, highlighting critical challenges in LLM preference assessment

## Impact

These findings highlight that LLMs do not uniformly uphold human rights principles, and that the language of interaction significantly influences model behavior. This work provides a foundation for improving rights-aware AI alignment.
